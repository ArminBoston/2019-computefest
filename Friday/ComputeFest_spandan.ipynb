{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broad Goal - Actionable Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Building a data driven machine learning pipeline to solve an actual business problem. \n",
    "- Building an intuition for machine learning methods when you approach new problem.\n",
    "- Give you a starting step to explore more advanced NLP models.\n",
    "- Project extensions for those interested in advanced learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of the Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshpop has 8 sections:\n",
    "\n",
    "1. Identifying a problem which ML can help solve.\n",
    "2. Approaching the problem in a data-driven manner. Formulating it as an ML problem.\n",
    "3. Data? Do we collect it all?\n",
    "4. Splitting the problem into sub-parts.\n",
    "4. Several ways to solve these sub-parts.\n",
    "5. What further can we do? What kind of model will be useful for this?\n",
    "6. Overview and Conclusions\n",
    "7. Proposed extensions for those interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. The problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a hypothetical firm XYZ Tech, which makes charging cables, mobile phone cases, and adapters. This is a crowded market, and they want to track people's reviews for improvement. However,large number of reviews make this hard. The CEO calls you in as a datascience to help solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. A Data driven approach to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data = Values of Variables.\n",
    "\n",
    "\n",
    "### Think about variables of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the important things the company would like to keep track of includes:-\n",
    "\n",
    "- Common complaints.\n",
    "- Deal breakers for people.\n",
    "- Thinks people like about the product.\n",
    "- New proposed feature requests.\n",
    "- Doing all of this for competitors products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed product to address this problem - An automated review analysis pipeline.\n",
    "\n",
    "What features might be good to have so that the output of the pipeline can be used to make business decisions?\n",
    "\n",
    "\n",
    "-  Ques) Negative reviews are more important for the company than positive ones. Can we identify negative ones?\n",
    "- Ans) This can be done using sentiment analysis methods, which allow you to categorize a review as positive or negative.\n",
    "\n",
    "\n",
    "\n",
    "- Ques) Reviews can often be a page long rants. What to do?\n",
    "- Ans) Let's automatically summarize reviews, i.e. give a 1-2 line short summary of each review, so that they can be read easily at a glance.\n",
    "\n",
    "\n",
    "\n",
    "- Ques)Would be good to have some structure to all these. Can we categorize them?\n",
    "- Ans) Yes! We can do classification to divide them into categories.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Ques)Can we get something more numeric or visual to compare?\n",
    "- Ans) Yes, ML can also help us visualize the content!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating this as a machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised, Unsupervised, or a blend of the two.\n",
    "\n",
    "- For the most part, we try to cast our problem in the form of a Supervised Machine Learning problem, as these techniques are well developed and understood. They usually tend to perform better, especially since the deep learning era. Curious about Why? We'll come to it.\n",
    "\n",
    "### Let's begin with a very quick refresher\n",
    "\n",
    "One good way to think about machine learning pipelines is that they take in some information, and convert it into more meaningful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slides/Images/machine_learning_intro.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ex:\n",
    "\n",
    "- Apple's FaceID takes a picture of a face and tells you if it matches a specific person's, i.e. Identification.\n",
    "\n",
    "- Amazon's Recommendation Engine takes in your purchase and web browising history and returns potential products you may want to buy. \n",
    "\n",
    "To create such systems, we usually resort to Supervised machine learning systems. Such a system takes in \"Pairs of X,y data\". In the above example, X corresponds to all the data we have collected on a user, and y corresponds to the product they ended up buying. By training a model using such data, given the data on a new user, we can begin to make predictions about what product they might be willing to buy!\n",
    "\n",
    "### So, let us try to design or Review Analysis Pipeline as a supervised machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a new review on Amazon, we would like to do the following:-\n",
    "\n",
    "<ol>\n",
    "<li> Tell if it is a positive or a negative review (Sentiment Analysis/Polarity Prediction).\n",
    "<li> For a negative review, summarize it (Sequence Summarization).\n",
    "<li> For all the negative reviews, identify groups of reviews (Clustering).\n",
    "<li> For each group, identify a representative sample for the management to read.\n",
    "</ol>\n",
    "\n",
    "As we proceed to build this pipeline, you will see that there are a lot of design decisions we need to make in our machine learning pipeline. One purpose of this tutorial is to lay these out for you, to help you get an overview of how to go about tackling a new problem using a machine learning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's talk about how to get the data for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for Sentiment Analysis\n",
    "\n",
    "We want pairs of data (X,y pairs), which look like this:-\n",
    "\n",
    "X = review of a product, y = whether it was a positive or a negative review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "        <link href=\"css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            <p>\n",
    "                For example:-<br>\n",
    "                (\"superb: I use it for my small business.\", Positive).<br>\n",
    "                (\"Broke within one month of use, I suggest against purchasing this charger\", Negative).\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "   </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for Sequence Summarization\n",
    "\n",
    "X = review of a product, y = summary of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:-<br>\n",
    "X= \"Not an \"ultimate guide\": Firstly,I enjoyed the format and tone of the book (how the author addressed the reader). However, I did not feel that she imparted any insider secrets that the book promised to reveal. If you are just starting to research law school, and do not know all the requirements of admission, then this book may be a tremendous help. If you have done your homework and are looking for an edge when it comes to admissions, I recommend some more topic-specific books. For example, books on how to write your personal statment, books geared specifically towards LSAT preparation (Powerscore books were the most helpful for me), and there are some websites with great advice geared towards aiding the individuals whom you are asking to write letters of recommendation. Yet, for those new to the entire affair, this book can definitely clarify the requirements for you.\"<br>\n",
    "\n",
    "y= \"Great for starting out, but if you've done your homework look for something more specific.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we want data for clustering of reviews, and identifying representative members of the cluster (3 and 4 above). These will be discussed later in greater detail as collecting data for them is not as straightforward. As a foreshadowing, I can tell you that using unsupervised machine learning, a lot can be done without collecting pairs of data. These are meant to give you an insight into what you can do, if you cannot collect pairs of X,y data due to difficulty/cost! Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, data collection is governed not only by the problem at hand, but by factors like:\n",
    "\n",
    "- What data is already publicly available and how it can be re-purposed for our problem.\n",
    "- How much will it cost to collect the data we need. Is there a way to collect data which solves a part/version of the problem we care about, and if that can help save a lot of money.\n",
    "- Are there any privacy concerns in collecting the data we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a thumb rule, start by looking at what is already available for free online. Chances are, there is an existing dataset that you can re-purpose for the problem you care about. If not completely, it can definitely be used for pilot testing before you go all in on the process of colecting data for your own purpose.\n",
    "\n",
    "### 3.1 Where to look for already existing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google it! Recently, google also made this - https://toolbox.google.com/datasetsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of sentiment analysis, there are several datasets that are already available online. The one which is closest to our purpose, is the Amazon Reviews Dataset.\n",
    "\n",
    "### 3.2 Amazon Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\n",
    "\n",
    "This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Re-formatting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, a big chunk of the time of anyone working with machine learning goes into re-formatting the dataset to put it in a form which can be fed into a machine learning pipeline.\n",
    "\n",
    "In this section, I am going to go over some of the different formats in which the data is often seen being stored. This includes:\n",
    "\n",
    "- As a combined text file for both X and y\n",
    "- As two seperate text files for the X (reviews) and y (their corresponding positive/negative labels).\n",
    "- As .csv files which can be opened in excel\n",
    "- In numeric formats you cannot open and read, for example as numpy arrays, or sparse scipy arrays (used very often)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As a combined Text file for X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a text file in python and read and store it's contents\n",
    "\n",
    "f = open('data/train.ft.txt','r')\n",
    "content = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
      "\n",
      "\n",
      "__label__2 The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\n",
      "\n",
      "\n",
      "__label__2 Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time's Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer's work (I haven't heard the Xenogears soundtrack, so I can't say for sure), and even if you've never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\n",
      "\n",
      "\n",
      "__label__2 Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\n",
      "\n",
      "\n",
      "__label__2 Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets see the first 5 lines.\n",
    "\n",
    "printed_count = 0\n",
    "while printed_count < 5:\n",
    "    c = content[printed_count]\n",
    "    c = c.rstrip()\n",
    "    print(c)\n",
    "    print('\\n')\n",
    "    printed_count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, 'label_2' corresponds to \"Positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reformatting the data into separate text files for X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/train_reviews.txt','w')\n",
    "f2 = open('data/train_labels.txt','w')\n",
    "\n",
    "for c in content:\n",
    "    c = c.rstrip()\n",
    "    words = c.split()\n",
    "    label = words[0]\n",
    "    review = ' '.join(words[1:])\n",
    "    print(review,file=f)\n",
    "    print(label,file=f2)\n",
    "\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_1 = \"Absolute love this charger's\"\n",
    "chunk_2 = \" advertisement!\"\n",
    "chunk_3 = \" Apart from it, not too special.\"\n",
    "chunk_4 = \" In fact, I'd say it's overpriced, and cheaper alternatives do a better.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reformatting into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec_path = 'data/GoogleNews-vectors-negative300.bin'\n",
    "model2 = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/train.ft.txt','r')\n",
    "content = f.readlines()\n",
    "f.close()\n",
    "\n",
    "data_rows = []\n",
    "for c in content:\n",
    "    c = c.strip()\n",
    "    words = c.split()\n",
    "    label = int(words[0].split('__')[-1])-1\n",
    "    text = ' '.join(words[1:])\n",
    "    row_data = [text,label]\n",
    "    data_rows.append(row_data)\n",
    "with open(\"data/train_data.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/test.ft.txt','r')\n",
    "content = f.readlines()\n",
    "f.close()\n",
    "\n",
    "data_rows = []\n",
    "for c in content:\n",
    "    c = c.strip()\n",
    "    words = c.split()\n",
    "    label = int(words[0].split('__')[-1])-1\n",
    "    text = ' '.join(words[1:])\n",
    "    row_data = [text,label]\n",
    "    data_rows.append(row_data)\n",
    "with open(\"data/test_data.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reformatting into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = open('data/train.ft.txt','r')\n",
    "content = f.readlines()\n",
    "f.close()\n",
    "\n",
    "all_reviews = []\n",
    "Y_Train = np.zeros(len(content))\n",
    "for i in range(len(content)):\n",
    "    c = content[i]\n",
    "    c = c.rstrip()\n",
    "    words = c.split(' ')\n",
    "    Y_Train[i] = int(words[0].split('__')[-1])-1\n",
    "    review = ' '.join(words[1:])\n",
    "    all_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import feature_extraction\n",
    "# tfidf_transformer = feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_Train = tfidf_transformer.fit_transform(X)\n",
    "\n",
    "# np.save('data/X_Train.npy', X_Train.toarray())\n",
    "# np.save('data/Y_Train.npy', Y_Train)\n",
    "\n",
    "# X_t = vectorize.fit_transform(all_reviews_test)\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_Test = tfidf_transformer.fit_transform(X_t)\n",
    "\n",
    "# X_Train_arr = np.load('X_Train.npy')\n",
    "# Y_Train = np.load('Y_Train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/test.ft.txt','r')\n",
    "content_test = f.readlines()\n",
    "f.close()\n",
    "\n",
    "all_reviews_test = []\n",
    "Y_Test = np.zeros(len(content_test))\n",
    "for i in range(len(content_test)):\n",
    "    c = content_test[i]\n",
    "    c = c.rstrip()\n",
    "    words = c.split(' ')\n",
    "    Y_Test[i] = int(words[0].split('__')[-1])-1\n",
    "    review = ' '.join(words[1:])\n",
    "    all_reviews_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize=CountVectorizer(max_df=0.95, min_df=0.005)\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorize.fit_transform(all_reviews)\n",
    "X_Train = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = vectorize.transform(all_reviews_test)\n",
    "X_Test = tfidf_transformer.transform(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/X_Train.npy', X_Train.toarray())\n",
    "np.save('data/Y_Train.npy', Y_Train)\n",
    "\n",
    "np.save('data/X_Test.npy', X_Test.toarray())\n",
    "np.save('data/Y_Test.npy', Y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600000, 1281)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 1281)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_Train = np.load('data/X_Train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Train = np.load('data/Y_Train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Test = np.load('data/X_Test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Test = np.load('data/Y_Test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Training models for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Sentences to Numbers: How Decision Boundaries relate to real world problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/hyperplane.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ever looked at such graphs and wondered how it relates to the sentences, or images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem - Computers only understand numbers. This text written here too is stored in computers in a numeric format. For instance, this sentence is seen by a computer as this numeric code - \n",
    "\n",
    "80 114 111 98 108 101 109 32 45 32 67 111 109 112 117 116 101 114 115 32 111 110 108 121 32 117 110 100 101 114 115 116 97 110 100 32 110 117 109 98 101 114 115 46 32 84 104 105 115 32 116 101 120 116 32 119 114 105 116 116 101 110 32 104 101 114 101 32 116 111 111 32 105 115 32 115 116 111 114 101 100 32 105 110 32 99 111 109 112 117 116 101 114 115 32 105 110 32 97 32 110 117 109 101 114 105 99 32 102 111 114 109 97 116 46 \n",
    "\n",
    "\n",
    "So, anything we deal with must be coverted into numbers! We convert any kind of data - text, images and even audio into numbers. And numbers can be put on a graph, that is how the dots in these figures correspond to numbers.\n",
    "\n",
    "The most important question that comes up then is - dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "In machine learning tongue, converting information into such numbers is called \"embedding them in space\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/embedding.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we do a 2D embedding. So, every dog is described by 2 numbers. These could be any 2 things, like their height, length, length of tail.\n",
    "\n",
    "We can also define them using the color of their eyes. But then, we'd need to define their color in numbers. How would we do that? ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the important point is, there's infinitely many ways to embed them. And the \"right one\" is the one which helps us solve the task at hand. For ex, if we want to divide dogs into tall and short ones, then, we just need 1 Dimension, their heights. If we want to divide them into fat and non fat ones, we would need their weight and height. Presumable, above a certain weight/height ratio we can call them fat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for our sentences, we have a 1281 Dimensional embedding right now. \n",
    "\n",
    "What does each axis represent?\n",
    "\n",
    "The presence or absence of a particular word!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 What is an SVM (in short)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to solve the problem for the hardest cases, and you will automatically learn the easier ones.\n",
    "\n",
    "For sentences, if you can make a system that can understand sarcastic reviews and say they are negative, it will be an easy job for the classifier to identify generic negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/hyperplane.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(max_iter=-1,C=0.01,class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize=CountVectorizer(max_df=0.95, min_df=0.005)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X = vectorize.fit_transform(all_reviews)\n",
    "X_Train = tfidf_transformer.fit_transform(X)\n",
    "\n",
    "X_t = vectorize.transform(all_reviews_test)\n",
    "X_Test = tfidf_transformer.transform(X_t)\n",
    "\n",
    "clf = SVC(max_iter=-1,C=0.01,class_weight='balanced')\n",
    "\n",
    "clf.fit(X_Train, Y_Train)\n",
    "\n",
    "predictions = clf.predict(X_Test)\n",
    "\n",
    "accuracy = np.sum(predictions == Y_Test)/len(Y_Test)\n",
    "print(\"Accuracy of SVM classifier\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Naive Bayes Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play a game to understand the intuition behind the naive bayes classifier.\n",
    "\n",
    "I will reveal this review in chunks, and with each chunk you need to enter your response in this poll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remarkable thing about about this charger is that it's company can sell a dummy through good marketing. It's absolutely useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, let's not get ahead of ourselves here. We know people mostly comment negative stuff, positive reviews are hardly given. But then, the word remarkable is something that's not used very often. Maybe we should give it more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_1 + chunk_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_1 + chunk_2 + chunk_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_1 + chunk_2 + chunk_3 + chunk_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intuition 1: The individual contributions of chunks to positive/negative probability can be used to get overall probability\n",
    "- Intuition 2: People usually tend to rate only negative products. Positive ones get less reviews.\n",
    "- Certain words are used more/less often. So we should should account for this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifnb = MultinomialNB()\n",
    "classifnb.fit(X_Train, Y_Train)\n",
    "\n",
    "predictions_nb=classifnb.predict(X_Test)\n",
    "\n",
    "accuracy_nb = np.sum(predictions_nb == Y_Test)/len(Y_Test)\n",
    "print(accuracy_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note - Naive? Equal importance to all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Mean word2vec + Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec: Giving every word an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "N = 3599973\n",
    "\n",
    "import gensim\n",
    "word2vec_path = 'data/GoogleNews-vectors-negative300.bin'\n",
    "model2 = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/train.ft.txt','r')\n",
    "content = f.readlines()\n",
    "f.close()\n",
    "\n",
    "all_reviews = []\n",
    "Y_Train = np.zeros(len(content))\n",
    "for i in range(len(content)):\n",
    "    c = content[i]\n",
    "    c = c.rstrip()\n",
    "    words = c.split(' ')\n",
    "    Y_Train[i] = int(words[0].split('__')[-1])-1\n",
    "    review = ' '.join(words[1:])\n",
    "    all_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/csail.mit.edu/u/s/smadan/miniconda3/envs/torch_tens/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "avg_wordvecs={}\n",
    "np_wordvecs=np.zeros((N,300))\n",
    "np_labels=np.zeros(N)\n",
    "ids = []\n",
    "for i in range(len(content)):\n",
    "    if i % 100000 == 0:\n",
    "        print(i)\n",
    "    c = content[i]\n",
    "    words = c.split(' ')\n",
    "    label = int(words[0].split('__')[-1])-1\n",
    "    count = 0\n",
    "    rep_sum = np.zeros(300)\n",
    "    for word in words[1:]:\n",
    "        if word in model2.vocab:\n",
    "            word_rep = model2[word]\n",
    "            rep_sum += word_rep\n",
    "            count+=1\n",
    "    review_rep = rep_sum/float(count)\n",
    "    if count > 0:\n",
    "        np_wordvecs[counter] = review_rep\n",
    "        np_labels[counter] = label\n",
    "        ids.append(i)\n",
    "        counter += 1\n",
    "\n",
    "X_Train = np_wordvecs\n",
    "Y_Train = np_labels\n",
    "np.save('X_Train_w2vec.npy', X_Train)\n",
    "np.save('Y_Train_w2vec.npy', Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = np.load('X_Train_w2vec.npy')\n",
    "Y_Train = np.load('Y_Train_w2vec.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x=torch.from_numpy(X_Train).float()\n",
    "tensor_y=torch.from_numpy(Y_Train).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/csail.mit.edu/u/s/smadan/miniconda3/envs/torch_tens/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.6815)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/csail.mit.edu/u/s/smadan/miniconda3/envs/torch_tens/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(0.6866)\n",
      "2 tensor(0.6882)\n",
      "3 tensor(0.6801)\n",
      "4 tensor(0.6697)\n",
      "5 tensor(0.6569)\n",
      "6 tensor(0.6435)\n",
      "7 tensor(0.6271)\n",
      "8 tensor(0.6106)\n",
      "9 tensor(0.5926)\n",
      "10 tensor(0.5739)\n",
      "11 tensor(0.5562)\n",
      "12 tensor(0.5388)\n",
      "13 tensor(0.5233)\n",
      "14 tensor(0.5095)\n",
      "15 tensor(0.4969)\n",
      "16 tensor(0.4866)\n",
      "17 tensor(0.4778)\n",
      "18 tensor(0.4699)\n",
      "19 tensor(0.4628)\n",
      "20 tensor(0.4569)\n",
      "21 tensor(0.4523)\n",
      "22 tensor(0.4488)\n",
      "23 tensor(0.4464)\n",
      "24 tensor(0.4441)\n",
      "25 tensor(0.4407)\n",
      "26 tensor(0.4374)\n",
      "27 tensor(0.4360)\n",
      "28 tensor(0.4346)\n",
      "29 tensor(0.4314)\n",
      "30 tensor(0.4285)\n",
      "31 tensor(0.4272)\n",
      "32 tensor(0.4251)\n",
      "33 tensor(0.4223)\n",
      "34 tensor(0.4207)\n",
      "35 tensor(0.4196)\n",
      "36 tensor(0.4175)\n",
      "37 tensor(0.4157)\n",
      "38 tensor(0.4148)\n",
      "39 tensor(0.4135)\n",
      "40 tensor(0.4118)\n",
      "41 tensor(0.4106)\n",
      "42 tensor(0.4099)\n",
      "43 tensor(0.4088)\n",
      "44 tensor(0.4074)\n",
      "45 tensor(0.4065)\n",
      "46 tensor(0.4059)\n",
      "47 tensor(0.4053)\n",
      "48 tensor(0.4044)\n",
      "49 tensor(0.4036)\n",
      "50 tensor(0.4029)\n",
      "51 tensor(0.4025)\n",
      "52 tensor(0.4021)\n",
      "53 tensor(0.4017)\n",
      "54 tensor(0.4012)\n",
      "55 tensor(0.4006)\n",
      "56 tensor(0.3999)\n",
      "57 tensor(0.3992)\n",
      "58 tensor(0.3986)\n",
      "59 tensor(0.3982)\n",
      "60 tensor(0.3978)\n",
      "61 tensor(0.3974)\n",
      "62 tensor(0.3971)\n",
      "63 tensor(0.3970)\n",
      "64 tensor(0.3974)\n",
      "65 tensor(0.3985)\n",
      "66 tensor(0.3995)\n",
      "67 tensor(0.3985)\n",
      "68 tensor(0.3953)\n",
      "69 tensor(0.3949)\n",
      "70 tensor(0.3966)\n",
      "71 tensor(0.3958)\n",
      "72 tensor(0.3937)\n",
      "73 tensor(0.3941)\n",
      "74 tensor(0.3949)\n",
      "75 tensor(0.3937)\n",
      "76 tensor(0.3926)\n",
      "77 tensor(0.3934)\n",
      "78 tensor(0.3935)\n",
      "79 tensor(0.3922)\n",
      "80 tensor(0.3918)\n",
      "81 tensor(0.3924)\n",
      "82 tensor(0.3920)\n",
      "83 tensor(0.3912)\n",
      "84 tensor(0.3911)\n",
      "85 tensor(0.3913)\n",
      "86 tensor(0.3910)\n",
      "87 tensor(0.3903)\n",
      "88 tensor(0.3902)\n",
      "89 tensor(0.3903)\n",
      "90 tensor(0.3901)\n",
      "91 tensor(0.3895)\n",
      "92 tensor(0.3893)\n",
      "93 tensor(0.3893)\n",
      "94 tensor(0.3894)\n",
      "95 tensor(0.3893)\n",
      "96 tensor(0.3888)\n",
      "97 tensor(0.3883)\n",
      "98 tensor(0.3882)\n",
      "99 tensor(0.3882)\n"
     ]
    }
   ],
   "source": [
    "# Code in file nn/two_layer_net_optim.py\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "losses=[]\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = N, 300, 100, 2\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(tensor_x)\n",
    "y = Variable(tensor_y,requires_grad=False)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # use a Classification Cross-Entropy loss\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Variables it should update.\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(100):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "    #print(x)\n",
    "    #print(y.type)\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "    a=loss[0][0][0][0]\n",
    "    #print(a)\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(np.asarray(losses))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'meanwordvec_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('meanwordvec_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_np_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(test_np_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/test.ft.txt','r')\n",
    "test_content = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "N2 = 399998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/csail.mit.edu/u/s/smadan/miniconda3/envs/torch_tens/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "test_np_wordvecs=np.zeros((N2,300))\n",
    "test_np_labels=np.zeros(N2)\n",
    "test_ids = []\n",
    "for i in range(len(test_content)):\n",
    "    c = test_content[i]\n",
    "    words = c.split(' ')\n",
    "    label = int(words[0].split('__')[-1])-1\n",
    "    count = 0\n",
    "    rep_sum = np.zeros(300)\n",
    "    for word in words[1:]:\n",
    "        if word in model2.vocab:\n",
    "            word_rep = model2[word]\n",
    "            rep_sum += word_rep\n",
    "            count+=1\n",
    "    review_rep = rep_sum/float(count)\n",
    "    if count > 0:\n",
    "        test_np_wordvecs[counter] = review_rep\n",
    "        test_np_labels[counter] = label\n",
    "        test_ids.append(i)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Test = test_np_wordvecs\n",
    "Y_Test = test_np_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399998"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_np_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(Y_Test)/len(Y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_Test_w2vec.npy', X_Test)\n",
    "np.save('Y_Test_w2vec.npy', Y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x_test=torch.from_numpy(X_Test).float()\n",
    "tensor_y_test=torch.from_numpy(Y_Test).long()\n",
    "\n",
    "x_test_tensor=Variable(tensor_x_test)\n",
    "predictions = model(x_test_tensor)\n",
    "np_preds=predictions.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "k=np.argpartition(np_preds,-K)[-K:]\n",
    "k=np.argsort(np_preds,axis=1)[:,-K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = Y_Test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(k[:,0] == gt)/len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8266991334956675\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy is',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at some of the errors made by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mask = k[:,0] == gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 9 9\n",
      "Actual Sentiment: Negative\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "Not an \"ultimate guide\": Firstly,I enjoyed the format and tone of the book (how the author addressed the reader). However, I did not feel that she imparted any insider secrets that the book promised to reveal. If you are just starting to research law school, and do not know all the requirements of admission, then this book may be a tremendous help. If you have done your homework and are looking for an edge when it comes to admissions, I recommend some more topic-specific books. For example, books on how to write your personal statment, books geared specifically towards LSAT preparation (Powerscore books were the most helpful for me), and there are some websites with great advice geared towards aiding the individuals whom you are asking to write letters of recommendation. Yet, for those new to the entire affair, this book can definitely clarify the requirements for you.\n",
      "\n",
      "False 45758 45757\n",
      "Actual Sentiment: Negative\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "Math Smart: I'm not too far into the book yet, but already I think it's great. The voice that the text is written in is so supportive, which may sound silly, but for people like me who are intimidated by math, this encouraging voice really helps. Things are explained well and the book makes me feel that I really can learn this stuff.\n",
      "\n",
      "False 91327 91326\n",
      "Actual Sentiment: Negative\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "\n",
      "RBF is just going through the motions: I've been a big fan of Reel Big Fish for a few years now. I enjoyed everything up to this record (including Cheer Up!, which I guess makes me one of the few), and this one has about three songs I genuinely listen to over and over again. It's more of the cynical/angry lyrics that really don't entertain me at all.If RBF hates being a band as much as they say, then perhaps they should end it now. If they release more stuff of this caliber, then it's just going to get worse.\n",
      "\n",
      "False 137375 137374\n",
      "Actual Sentiment: Positive\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "Too complicated: I am a great fan of SF in general and Niven in particular. I love most of his other books, especially the Ringworld series, the Mote In God's Eye, World Out of Time, and all the Known Space short tales.However, this book got me lost. I just couldn't get \"into\" it. Too alien, too complicated. I tried a few times to read it, but just couldn't. I got stuck after a few pages.\n",
      "\n",
      "False 182634 182633\n",
      "Actual Sentiment: Positive\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "But where's the kids?: Maurice Sendak makes even the most bitter jaded adults kids again. You need the children singing it to really enjoy it. With the kids, it's 5 stars and happiness. Without it, it's just Carole King singing Maurice Sendak. Okay, but missing something.The songs are absolutely wonderful as always. BUT, what I really want is the original with the kids. Until then, pass on this.\n",
      "\n",
      "False 229401 229400\n",
      "Actual Sentiment: Positive\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "Amazing performance, nauseating editing: I LOVE Adele, and her performance is amazing, but impossible to watch on this blu-ray or DVD. As other reviewers have mentioned, the constant cuts every few seconds are nauseating. You can't even focus your eyes on anything, because the views change so fast. What a shame. Hopefully, Adele's next concert video will have a different editor/director who will let us watch her mesmerizing performance.\n",
      "\n",
      "False 275160 275159\n",
      "Actual Sentiment: Positive\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "High quality at good price: A quality product even the scabbard(which is something I have found companies to skimp on)is of quality.The dirk arrived earlier than originally expected (two days after the order was placed).It was coated in a clean grease and wrapped in plastic to prevent rusting. Upon inspecting the dirk I found it to be of good quality and the details on it are slightly imperfect but I think gives it a character of its own(almost as if it were hand made).for the amount I paid I think it is a very good value for my hard earned money.\n",
      "\n",
      "False 321336 321335\n",
      "Actual Sentiment: Positive\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "Gimmick: This filter may be adequate if your water issues sparkling from a good well. But if you are using a municipal water supply, forget it. My coffee tasted exactly the same using this filter as it did without one at all--like chlorine! And these little things are pricey. Makes a lot more sense to purchase a water filter pitcher. That way you kill two birds with one stone--clean drinking water AND flavorful coffee.\n",
      "\n",
      "False 368594 368593\n",
      "Actual Sentiment: Positive\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "The First Joss Whedon Show that Deserved to be Canceled: The first three episodes are unwatchable, an asexual sex fantasy starring maybe the worst actress working today. Long live Buffy and Firefly, two of the best shows ever aired, but this thing looks to be an abortion of digital media.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printed = -1\n",
    "id_label = {'0':'Negative','1':'Positive'}\n",
    "for i in range(len(true_mask)):\n",
    "    if true_mask[i] == False:\n",
    "        printed += 1\n",
    "        if printed % 8000 ==0: \n",
    "            test_id = test_ids[i]\n",
    "            print(true_mask[i],test_id,i)\n",
    "            print('Actual Sentiment: %s'%id_label[str(int(test_np_labels[test_id]))])\n",
    "            print('Predicted Sentiment: %s'%id_label[str(int(k[:,0][test_id]))])\n",
    "            print('\\n')\n",
    "            words = test_content[test_id].split(' ')\n",
    "            print(' '.join(words[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions from comparing different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you can see, the performance of the three models here was not very different. But, how is this possible? Shouldn't  the neural network beat everything else?\n",
    "\n",
    "Well, it could. Maybe, maybe not. \n",
    "\n",
    "The larger point I am trying to make here is to not rush to a complicated neural network, unless needed. The Binary classification task of positive vs negative is in fact an easy task. Also, a simple, specialized model lik the naive bayes seems to do just fine for this task. So, we probably should just stick to a simple model which is fast, easy and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Negative review summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Overview and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Proposed extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Named entity extraction\n",
    "- Identifying feature requests\n",
    "- Exploring what our networks have learned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
