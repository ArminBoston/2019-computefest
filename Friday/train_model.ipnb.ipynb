{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2 :Training\n",
    "\n",
    "In this notebook, we are going to build a sequence to sequence model(with and without attention) using Keras using the preprocessed data from the earlier notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import gensim as gs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.layers import Dense,LSTM,Input,Activation,Add,TimeDistributed,\\\n",
    "Permute,Flatten,RepeatVector,merge,Lambda,Multiply,Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read embeddings and preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = 'E:\\\\Spring-19\\\\Workshop\\\\saved_models'\n",
    "FN = 'embeddings.pkl'\n",
    "\n",
    "with open(os.path.join(pickle_path,FN), 'rb') as fp:\n",
    "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
    "vocab_size, embedding_size = embedding.shape\n",
    "\n",
    "FN = 'data.pkl'\n",
    "with open(os.path.join(pickle_path,FN), 'rb') as fp:\n",
    "    X, Y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparams. We will visit these parameters when we use them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "LR = 1e-4\n",
    "batch_size=64\n",
    "\n",
    "maxlend=25 \n",
    "maxlenh=25\n",
    "maxlen = maxlend + maxlenh\n",
    "batch_norm=False\n",
    "\n",
    "val_samples = 3000\n",
    "emb_size = 100\n",
    "hidden_units= emb_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create multiple out of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = 0\n",
    "eos = 1\n",
    "idx2word[empty] = '_'\n",
    "idx2word[eos] = '~'\n",
    "\n",
    "nb_unknown_words = 10\n",
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size-1-i] = '<%d>'%i\n",
    "\n",
    "oov0 = vocab_size-nb_unknown_words\n",
    "\n",
    "for i in range(oov0, len(idx2word)):\n",
    "    idx2word[i] = idx2word[i]+'^'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide into train and val samples preferably in multiples of batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35170, 32136, 32136, 3034, 3034)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_batches = len(X) // batch_size\n",
    "num_val_samples = val_samples + len(X) - batch_size*num_train_batches\n",
    "num_val_batches = num_val_samples // batch_size\n",
    "total_entries = (num_train_batches + num_val_batches)*batch_size\n",
    "X, Y = X[:total_entries], Y[:total_entries]\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size= num_val_samples , random_state=seed)\n",
    "len(X), len(X_train), len(Y_train), len(X_val), len(Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couple of helper functions\n",
    "\n",
    "##### Attach EOS and pre-pad  : \n",
    "left (pre) pad a description to maxlend and then add eos. The eos is the input to predicting the first word in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpadd(x, maxlend=maxlend, eos=eos):\n",
    "    assert maxlend >= 0\n",
    "    if maxlend == 0:\n",
    "        return [eos]\n",
    "    n = len(x)\n",
    "    if n > maxlend:\n",
    "        x = x[-maxlend:]\n",
    "        n = maxlend\n",
    "    return [empty]*(maxlend-n) + x + [eos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign unknown words\n",
    "1. Convert list of word indexes that may contain words outside vocab_size to words inside.\n",
    "2. If a word is outside, try first to use glove_idx2idx to find a similar word inside.\n",
    "3. If none exist then replace all accurancies of the same unknown word with <0>, <1>, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_fold(xs):\n",
    "    xs = [x if x < oov0 else glove_idx2idx.get(x,x) for x in xs]\n",
    "    # the more popular word is <0> and so on\n",
    "    outside = sorted([x for x in xs if x >= oov0])\n",
    "    # if there are more than nb_unknown_words oov words then put them all in nb_unknown_words-1\n",
    "    outside = dict((x,vocab_size-1-min(i, nb_unknown_words-1)) for i, x in enumerate(outside))\n",
    "    xs = [outside.get(x,x) for x in xs]\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE DATA BATCH GENERATOR\n",
    "\n",
    "We do use teacher forcing(https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/) in our model and hence the input sentences to the decoder will be lagging the true sentence to be deocded by one string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(Xd, Xh, batch_size=batch_size, nb_batches=None, model=None, seed=seed):\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        xds = []\n",
    "        xhs = []\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        for b in range(batch_size):\n",
    "            t = random.randint(0,len(Xd)-1)\n",
    "            \n",
    "            #random shuffling of data\n",
    "            xd = Xd[t]\n",
    "            s = random.randint(min(maxlend,len(xd)), max(maxlend,len(xd)))\n",
    "            xds.append(xd[:s])\n",
    "            \n",
    "            xh = Xh[t]\n",
    "            s = random.randint(min(maxlenh,len(xh)), max(maxlenh,len(xh)))\n",
    "            xhs.append(xh[:s])\n",
    "\n",
    "        # undo the seeding before we yield inorder not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(xds, xhs)\n",
    "\n",
    "def conv_seq_labels(xds, xhs, nflips=None, model=None):\n",
    "    \"\"\"description and hedlines are converted to padded input vectors. headlines are one-hot to label\"\"\"\n",
    "    batch_size = len(xhs)\n",
    "    \n",
    "    x = [vocab_fold(lpadd(xd)+xh) for xd,xh in zip(xds,xhs)]  # the input does not have 2nd eos\n",
    "    x = sequence.pad_sequences(x, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "        \n",
    "    y = np.zeros((batch_size, maxlenh, vocab_size))\n",
    "    for i, xh in enumerate(xhs):\n",
    "        xh = vocab_fold(xh) + [eos] + [empty]*maxlenh  # output does have a eos at end\n",
    "        xh = xh[:maxlenh]\n",
    "        y[i,:,:] = np_utils.to_categorical(xh, vocab_size)\n",
    "        \n",
    "    #The 3 inputs are description, summary starting with eos and a one-hot encoding of the summary categorical variables.\n",
    "    return [x[:,:maxlend],x[:,maxlend:]], y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some sanity check to see what the processing yields us. See that the summary input into the model is beginning with a '~'trigger always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 25) (64, 25) (64, 25, 40000)\n",
      "Description  :  ['of', 'their', 'poppers', 'and', 'popcorn.', 'Shipping', 'was', 'quick', 'and', 'the', 'popcorn', 'was', 'awesome.', 'Love', 'how', 'simple', 'to', 'make', 'the', 'popcorn', 'with', 'all', 'the', '<0>^', 'in']\n",
      "Summary  :  ['~', 'Happy', 'with', 'their', 'products', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "r = next(gen(X_train, Y_train, batch_size=batch_size))\n",
    "print(r[0][0].shape,r[0][1].shape,r[1].shape)\n",
    "print(\"Description  : \", [idx2word[k] for k in r[0][0][5]])\n",
    "print(\"Summary  : \", [idx2word[k] for k in r[0][1][5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = gen(X_train, Y_train, batch_size=batch_size)\n",
    "val_gen =  gen(X_val, Y_val, nb_batches=val_samples//batch_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "clip_norm = 1.0\n",
    "regularizer = l2(weight_decay) if weight_decay else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder(train_gen, val_gen, mode = 'fit', num_epochs=1,en_shape=maxlend,de_shape=maxlenh):\n",
    "    \n",
    "    print('Encoder_Decoder LSTM...')\n",
    "   \n",
    "    \"\"\"__encoder___\"\"\"\n",
    "    encoder_inputs = Input(shape=(en_shape,))\n",
    "    print(encoder_inputs)\n",
    "    \n",
    "    #APPLY EMBEDDING LAYER.        \n",
    "    input_emb = Embedding(vocab_size, embedding_size,\n",
    "                    input_length=maxlend,\n",
    "                    W_regularizer=regularizer, dropout=p_emb, weights=[embedding], mask_zero=True,\n",
    "                    name='embedding_1')\n",
    "    \n",
    "    #ENCODER LSTM - FORWARD     \n",
    "    encoder_LSTM = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2 ,return_state=True)\n",
    "    encoder_LSTM_rev=LSTM(hidden_units,return_state=True,go_backwards=True)\n",
    "    \n",
    "    #ENCODER LSTM - REVERSE\n",
    "    encoder_outputsR, state_hR, state_cR = encoder_LSTM_rev(input_emb(encoder_inputs))\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(input_emb(encoder_inputs))\n",
    "        \n",
    "    state_hfinal=Add()([state_h,state_hR])\n",
    "    state_cfinal=Add()([state_c,state_cR])\n",
    "    \n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "    \n",
    "    \"\"\"____decoder___\"\"\"\n",
    "    #Input to the decoder would be the summary(headline) sequence starting from ~ character.\n",
    "    decoder_inputs = Input(shape=(de_shape,))\n",
    "    \n",
    "    print(decoder_inputs)\n",
    "      \n",
    "    decoder_LSTM = LSTM(hidden_units,return_sequences=True,return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(input_emb(decoder_inputs),initial_state=encoder_states) \n",
    "    decoder_dense = Dense(de_shape,activation='linear')\n",
    "    \n",
    "    # Apply a dense layer that has vocab_size(40000) outputs which learns probability of each word when softmax is applied.\n",
    "    # TimeDistributed is a wrapper for applying the same function over all the time step outputs. \n",
    "    # Refer https://keras.io/layers/wrappers/\n",
    "    time_distributed = TimeDistributed(Dense(vocab_size,\n",
    "                                W_regularizer=regularizer, b_regularizer=regularizer,\n",
    "                                name = 'timedistributed_1'))\n",
    "    activation = Activation('softmax', name='activation_1')\n",
    "    decoder_outputs = activation(time_distributed(decoder_outputs))\n",
    "    \n",
    "    #Model groups layers into an object with training and inference features.\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/keras/models/Model        \n",
    "    model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
    "    \n",
    "    rmsprop = RMSprop(lr=learning_rate,clipnorm=clip_norm)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=rmsprop)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.fit_generator(train_gen,\n",
    "                  steps_per_epoch = num_train_batches,\n",
    "                  epochs=5,  #Try different epochs as hyperparameter                \n",
    "                  validation_data = val_gen,\n",
    "                  validation_steps = num_val_batches\n",
    "                           )\n",
    "    \n",
    "    #_________________________INFERENCE MODE______________________________#  \n",
    "    \n",
    "    encoder_model_inf = Model(encoder_inputs,encoder_states)\n",
    "    \n",
    "    decoder_state_input_H = Input(shape=(hidden_units,))\n",
    "    decoder_state_input_C = Input(shape=(hidden_units,)) \n",
    "    decoder_state_inputs = [decoder_state_input_H, decoder_state_input_C]\n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_LSTM(input_emb(decoder_inputs),\n",
    "                                                                     initial_state=decoder_state_inputs)\n",
    "    decoder_states = [decoder_state_h, decoder_state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model_inf= Model([decoder_inputs]+decoder_state_inputs,\n",
    "                         [decoder_outputs]+decoder_states)\n",
    "    \n",
    "    return model,encoder_model_inf,decoder_model_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "\n",
    "Implement multiple layers in LSTM. Right now, we have implemented single layer LSTMs only. For appropriate performance, implement 2/3 layers LSTMS with 128/256 size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModels(models,names):\n",
    "    path = 'E:\\\\Spring-19\\\\Workshop\\\\saved_models'\n",
    "    for i in range(len(names)):\n",
    "        models[i].save(os.path.join(path,names[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model, encoder, decoder = encoder_decoder(train_gen, val_gen)\\nsaveModels([model,encoder,decoder],[\"init_model\",\"encoder\",\"decoder\"])'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model, encoder, decoder = encoder_decoder(train_gen, val_gen)\n",
    "saveModels([model,encoder,decoder],[\"init_model\",\"encoder\",\"decoder\"])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_with_attention(train_gen, val_gen, mode = 'fit', num_epochs=1,en_shape=maxlend,de_shape=maxlenh):\n",
    "    encoder_inputs = Input(shape=(en_shape,))\n",
    "    print(encoder_inputs)\n",
    "        \n",
    "    input_emb = Embedding(vocab_size, embedding_size,\n",
    "                    input_length=maxlend,\n",
    "                    W_regularizer=regularizer, dropout=p_emb, weights=[embedding], mask_zero=False,\n",
    "                    name='embedding_1')\n",
    "    \n",
    "    encoder_LSTM = LSTM(hidden_units,dropout_U=0.2,dropout_W=0.2,return_sequences=True,return_state=True)\n",
    "    encoder_LSTM_rev=LSTM(hidden_units,return_state=True,return_sequences=True,dropout_U=0.05,dropout_W=0.05,go_backwards=True)\n",
    "    \n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(input_emb(encoder_inputs))\n",
    "    encoder_outputsR, state_hR, state_cR = encoder_LSTM_rev(input_emb(encoder_inputs))\n",
    "    \n",
    "    state_hfinal=Add()([state_h,state_hR])\n",
    "    state_cfinal=Add()([state_c,state_cR])\n",
    "    encoder_outputs_final = Add()([encoder_outputs,encoder_outputsR])\n",
    "    \n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "    \n",
    "    decoder_inputs = Input(shape=(de_shape,))\n",
    "    print(decoder_inputs)\n",
    "    decoder_LSTM = LSTM(hidden_units,return_sequences=True,dropout_U=0.2,dropout_W=0.2,return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(input_emb(decoder_inputs),initial_state=encoder_states)\n",
    "    \n",
    "    ######################ATTENTION####################################################\n",
    "    \n",
    "    attention = TimeDistributed(Dense(1, activation = 'tanh'))(encoder_outputs_final)\n",
    "    attention = Multiply()([attention,decoder_outputs])\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "    \n",
    "    ####################################################################################\n",
    "    time_distributed = TimeDistributed(Dense(vocab_size,\n",
    "                                W_regularizer=regularizer, b_regularizer=regularizer,\n",
    "                                name = 'timedistributed_1'))\n",
    "    activation = Activation('softmax', name='activation_1')\n",
    "    decoder_outputs = activation(time_distributed(decoder_outputs))\n",
    "    \n",
    "    model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
    "    \n",
    "    rmsprop = RMSprop(lr=learning_rate,clipnorm=clip_norm)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=rmsprop)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.fit_generator(train_gen,\n",
    "                  steps_per_epoch = num_train_batches,\n",
    "                  epochs=1,                  \n",
    "                  validation_data = val_gen,\n",
    "                  validation_steps = num_val_batches\n",
    "                           )\n",
    "        \n",
    "    #########################INFERENCE###################################    \n",
    "    encoder_model_inf = Model(encoder_inputs,encoder_states)\n",
    "    \n",
    "    decoder_state_input_H = Input(shape=(en_shape,))\n",
    "    decoder_state_input_C = Input(shape=(en_shape,)) \n",
    "    decoder_state_inputs = [decoder_state_input_H, decoder_state_input_C]\n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_LSTM(input_emb(decoder_inputs),\n",
    "                                                                     initial_state=decoder_state_inputs)\n",
    "    decoder_states = [decoder_state_h, decoder_state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model_inf= Model([decoder_inputs]+decoder_state_inputs,\n",
    "                         [decoder_outputs]+decoder_states)\n",
    "    \n",
    "    return model, encoder_model_inf, decoder_model_inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model, encoder, decoder = encoder_decoder_with_attention(train_gen,val_gen)\\nsaveModels([model,encoder,decoder],['att_model','att_encoder','att_decoder'])\""
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model, encoder, decoder = encoder_decoder_with_attention(train_gen,val_gen)\n",
    "saveModels([model,encoder,decoder],['att_model','att_encoder','att_decoder'])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HOW TO DECODE FOR INFERENCE ?\n",
    "\n",
    "There are two standard options to decode while doing inference. While doing inference, remember that you don't have access to ground truth summaries and you have to build your own summaries based on the words that the decoder predicted already. While training, we used teacher forcing where we fed in the ground truth word from the previous time-stamp - not feasible during inference.\n",
    "\n",
    "1. Be Greedy - Feed the previously predicted word into the decoder as the input for the next time step. Remember we used teacher forcing during training where we used the ground truth label of the previous time step.\n",
    "\n",
    "2. Use Beam Search of size K - Retain top (in terms of log likelihood) K alternatives whenever you decode each new word and append to existing sequences. Better performing since it allows the model to recover from errors. More costly to run inference.\n",
    "\n",
    "Example\n",
    "\n",
    "#### TRUE SUMMARY \n",
    "Treat for pet lovers.\n",
    "\n",
    "#### GREEDY PREDICTIONS\n",
    "\n",
    "Treat\n",
    "\n",
    "Treat yourself \n",
    "\n",
    "Treat yourself for\n",
    "\n",
    "Treat yourself for dog\n",
    "\n",
    "#### BEAM SEARCH (size 4 beam)\n",
    "\n",
    "Treat, Yummy, Select, Best\n",
    "\n",
    "Treat for, Yummy flavor, Treat yourself, Yummy !\n",
    "\n",
    "Treat for dog, Treat for pet, Treat for overweight, Yummy flavor for\n",
    "\n",
    "Treat for dog callers, Treat for dog owners, Treat for pet diseases, Treat for dog collars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test time inference decoder example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note for Later : Try writing your own decoder both using greedy search and beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming article is input as a pre-processed X (padded etc. using earlier functions). Sample code for decoding.\n",
    "def summarize(article,encoder,decoder):\n",
    "    summary = []\n",
    "    article = np.reshape()\n",
    "    article =  np.reshape(article,(1,en_shape))\n",
    "    \n",
    "    encoded_state_val = encoder.predict(article)\n",
    "    target_seq = np.zeros((1,1,vocab_size))\n",
    "    \n",
    "    while len(summary) < de_shape:\n",
    "        decoder_out,decoder_h,decoder_c= decoder.predict(x=[target_seq]+init_state_val)\n",
    "        summary.append(decoder_out)\n",
    "        init_state_val= [decoder_h,decoder_c]\n",
    "        #get most similar word and put in line to be input in next timestep\n",
    "        #target_seq=np.reshape(model.wv[getWord(decoder_out)[0]],(1,1,emb_size_all))\n",
    "        target_seq=np.reshape(decoder_out,(1,1,de_shape[1]))    \n",
    "    \n",
    "    return summary    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME SUMMARIZATION EXAMPLES\n",
    "\n",
    "<small>These are examples collected from the model run on the reviews dataset (Remember we gave you a trimmed version) using multi-layer LSTMS on K80 GPUs. Here we present some results since they make for interesting discussions and viewing.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
